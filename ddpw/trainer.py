import os
import abc
from enum import Enum
from typing import final
from dataclasses import dataclass

import torch
from torch.utils import data

from .utils import Utils
from .platform import PlatformConfig
from .artefacts import ArtefactsConfig


@final
class TrainingMode(Enum):
  r"""Modes of running the task."""

  TRAIN = 0
  r"""The task is to train."""

  RESUME = 1
  r"""The task is to resume training from a checkpoint."""

  EVALUATE = 2
  r"""The task is to evaluate a model checkpoint (`i.e.`, a trained model)."""


@final
@dataclass
class TrainingConfig(object):
  job_type: TrainingMode = TrainingMode.TRAIN
  r"""Type of job. Default: ``TrainingMode.TRAIN``."""

  start_at: int = 0
  r"""Epoch number from which to start training. Default: ``0``."""

  epochs: int = 25
  r"""Epoch number at which to stop training. Default: ``25``."""

  model_name_prefix: str = 'model'
  r"""Prefix for saving the model. Default: ``model``."""

  model_path: str = './model'
  r"""Location at which to store the model. Default: ``./model``."""

  checkpoint_name_prefix: str = 'ckpt'
  r"""Prefix for saving the checkpoint. Default: ``ckpt``."""

  checkpoint_path: str = './checkpoint'
  r"""Location at which to store the checkpoint. Default: ``./checkpoint``."""

  console_logs_path: str = './logs/console_logs'
  r"""Location at which to store console logs. (`e.g.`, logs generated by
    SLURM). Default: ``./logs/console_logs``."""

  training_logs_path: str = './logs/training_logs'
  r"""Location at which to store training logs (`e.g.`, logs from Tensorboard).
  Default: ``./logs/training_logs``."""

  save_every: int = 5
  r"""Save a checkpoint every few epochs. If 0, ignored. Default: ``5``."""


@dataclass
class Trainer(object):
  r"""
  This is a template class with abstract methods to be defined by the user.
  This class provides methods to define training and evaluation methods. Once
  the wrapper has moved the model and the dataset to the appropriate device, it
  calls :py:meth:`.train()` or :py:meth:`.evaluate()` as configured.
  """

  p_config: PlatformConfig = None
  r"""
  .. admonition:: Definition not required
   :class: note

   This property need not be specified by the user and will be automatically
   updated by the wrapper right before training or evaluation. This can be
   directly accessed in the :py:meth:`~Trainer.train` and
   :py:meth:`~Trainer.evaluate` methods.

  Platform-related configuration. This property may be used in the training
  and evaluation methods to access platform-related information such as if the
  platform is on CUDA, how big the world is, whether synchronisation across
  devices is needed, `etc`."""

  t_config: TrainingConfig = None
  r"""Training-related configurations. This property may be used in the training
  and evaluation methods to access training-specific aspects such as the number
  of training epochs, epoch interval to store the training state, `etc`."""

  artefacts: ArtefactsConfig = None
  r"""
  .. admonition:: Definition not required
   :class: note

   This property need not be specified by the user and will be automatically
   updated by the wrapper right before training or evaluation. This can be
   directly accessed in the :py:meth:`~Trainer.train` and
   :py:meth:`~Trainer.evaluate` methods.

  Model-related configuration. This property may be used in the training and
  evaluation methods to access models, datasets, optimisation stragegy (the
  optimiser), objective (energy/loss functions) `etc.`"""

  @abc.abstractmethod
  def train(self, global_rank: int):
    r"""
    .. admonition:: Definition required
      :class: important

      This method needs to be explicitly defined by the user.

    This method provides definition for the training procedure.

    :param int global_rank: Global rank of the current device.

    :raises NotImplementedError: Training has not been implemented.
    """

    raise NotImplementedError

  @abc.abstractmethod
  def evaluate(self, global_rank: int, dataset: data.DataLoader):
    r"""
    .. admonition:: Definition required
      :class: important

      This method needs to be explicitly defined by the user.

    This method provides definition for the evaluation procedure.

    :param int global_rank: Global rank of the current device.
    :param data.DataLoader dataset: The dataset to use for evaluation.

    :raises NotImplementedError: Evaluation has not been implemented.
    """

    raise NotImplementedError

  def save_state(self, epoch: int):
    r"""
    This method saves the state of the training, such as the model parameters,
    optimiser gradients, the current epoch, `etc`., and can be called at every
    few epochs (as specified in :py:attr:`.TrainingConfig.save_every`) or as
    needed. Override this method to save more information. The state so saved is
    used by the :py:meth:`.__restore_state` method that is called to resume from
    a checkpoint or evaluate a model.

    :param int epoch: The epoch number at which to save the training state.
    """

    checkpoint = {
      'stopped_at': epoch,
      'model': self.artefacts.model.state_dict(),
      'optimiser': self.artefacts.optimiser.state_dict()
    }
    torch.save(checkpoint, os.path.join(self.t_config.checkpoint_path,
                          f'{self.t_config.checkpoint_name_prefix}_{epoch}.pt'))

  def __restore_state(self, resume_at: int):
    r"""
    Restore training from a saved state.

    :param int resume_at: The epoch checkpoint whence to resume training.
    """

    filename = f'{self.t_config.checkpoint_name_prefix}_{resume_at}.pt'
    file_path = os.path.join(self.t_config.checkpoint_path, filename)
    assert os.path.isfile(file_path)

    Utils.print(f'Loading model at {file_path}.')
    checkpoint = torch.load(file_path)
    self.t_config.save_every = checkpoint['stopped_at']
    self.artefacts.model.load_state_dict(checkpoint['model'])
    self.artefacts.optimiser.load_state_dict(checkpoint['optimiser'])

  def save_model(self, dst: str = None):
    r"""
    The method saves the model parameters at the specified destination.

    :param str dst: Destination filename. If not specified, the path is formed
      from :py:attr:`.TrainingConfig.model_path` plus
      :py:attr:`.TrainingConfig.model_name_prefix`.
    """

    if dst is None:
      dst = os.path.join(self.t_config.model_path,
                          f'{self.t_config.model_name_prefix}.pt')

    torch.save(self.artefacts.model.state_dict(), dst)

  def load_model(self, src: str = None):
    r"""
    This method loads a model's parameters at the specified path.

    :param str src: Source filename. If not specified, the path is formed
      from :py:attr:`.TrainingConfig.model_path` plus
      :py:attr:`.TrainingConfig.model_name_prefix`.
    """

    if src is None:
      src = os.path.join(self.t_config.model_path,
                         f'{self.t_config.model_name_prefix}.pt')

    assert os.path.exists(src)

    self.artefacts.model.load_state_dict(torch.load(src))

  def __call__(self, global_rank: int):
    r"""
    When once the distributed data parallel setups are completed by the wrapper,
    this method is called. This method locally updates the dataset and model
    allotted for the current GPU in case of GPU- and SLURM-based platforms.

    :param global_rank int: The global rank of the device.
    """

    Utils.print(
      f'[Device {global_rank}] Copying model parameters to the optimiser.')
    self.artefacts.optimiser = self.artefacts.optimiser_config(
      self.artefacts.model)

    # if this task is resumption from or evaluation of a saved model, load it
    if self.t_config.job_type in [TrainingMode.RESUME, TrainingMode.EVALUATE]:
      Utils.print(f'[Device {global_rank}] Model load setup underway.')
      self.__restore_state(self.t_config.start_at)

    # whether to training (or resumption) or evaluate
    if self.t_config.job_type in [TrainingMode.TRAIN, TrainingMode.RESUME]:
      self.train(global_rank)
    else:
      self.evaluate(global_rank, self.artefacts.test_set)
